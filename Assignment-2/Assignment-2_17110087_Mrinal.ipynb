{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,SimpleRNN \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as utils\n",
    "import re\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import math\n",
    "import codecs\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Approach - N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "f = codecs.open('speech.txt', 'r', 'UTF-8')\n",
    "raw_text = f.read()\n",
    "text_data = text_data.replace(\"SPEECH\", \"\")\n",
    "text_data = text_data.replace(\"\\'\", \"\")\n",
    "text_data = text_data.replace(\",\", \"\")\n",
    "text_data = text_data.replace(\"\\r\\n\", \"\")\n",
    "text_data = re.sub('[0-9]', r'', text_data)\n",
    "text = text_data\n",
    "sents = sent_tokenize(text)\n",
    "new_list = []\n",
    "for sent in sents:\n",
    "    sent = re.sub(r'\\[(?:[^\\]|]*\\|)?([^\\]|]*)\\]', r'\\1', sent)\n",
    "    sent = re.sub('[\\\"]', r'',sent)\n",
    "    sent = re.sub('[.]', r'',sent)\n",
    "    new_list.append('<s> ' + sent.lower() + ' </s>')\n",
    "    \n",
    "def predict(text, N, freq_count):\n",
    "    \n",
    "    token_seq = ' '.join(text.split()[-(N-1):])\n",
    "    if N!=1:\n",
    "        choices = freq_count[token_seq].items()\n",
    "    else:\n",
    "        choices = freq_count[''].items()\n",
    "    pvals=[]\n",
    "    key_words=[]\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    \n",
    "    for key,values in choices:\n",
    "        key_words.append(key)\n",
    "        pvals.append(values/total)\n",
    "        \n",
    "    r = np.random.multinomial(3, pvals, size=None)\n",
    "    req_index = np.argmax(r)\n",
    "    choice = key_words[req_index]\n",
    "    \n",
    "    return choice\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngrams(sent_list, N):\n",
    "\n",
    "    ngrams = []\n",
    "    for sent in sent_list:\n",
    "        tokens = sent.split()\n",
    "        for i in range(len(tokens)-N+1):\n",
    "            ngrams.append(tokens[i:i+N])\n",
    "\n",
    "    freq_count = {}\n",
    "    \n",
    "    for i in ngrams:\n",
    "        token_seq  = ' '.join(i[:-1])\n",
    "        last_token = str(i[-1])\n",
    "\n",
    "        if token_seq not in freq_count:\n",
    "            freq_count[token_seq] = {};\n",
    "        \n",
    "        if last_token not in freq_count[token_seq]:\n",
    "            freq_count[token_seq][last_token] = 0;\n",
    "\n",
    "        freq_count[token_seq][last_token] += 1;\n",
    "        \n",
    "    return freq_count\n",
    "   \n",
    "def generator(N, freq_count):\n",
    "    start_seq=None\n",
    "    start_tag_list=[]\n",
    "    \n",
    "    for i in freq_count.keys():\n",
    "        a = i.split()\n",
    "        if N!=1 and a[0]=='<s>':\n",
    "            start_tag_list.append(i)\n",
    "    \n",
    "    if(start_seq is None) and N!=1: \n",
    "        start_seq = random.choice(start_tag_list);\n",
    "    elif(start_seq is None) and N==1:\n",
    "        start_seq=\"<s>\"\n",
    "    rand_text = start_seq.lower();\n",
    "\n",
    "    sentences = 0;\n",
    "    \n",
    "    next_word = ''\n",
    "    \n",
    "    while next_word!= '</s>':\n",
    "        next_word = predict(rand_text, N, freq_count)\n",
    "        rand_text += ' ' + next_word\n",
    "        \n",
    "    return rand_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      " Printing output \n",
      "\n",
      "<s> it’s going to be first </s>\n",
      "<s> an incredible company </s>\n",
      "<s> doctors are quitting the business </s>\n",
      "<s> youre not going to be an amazing two monthswe might not even need the rhetoric </s>\n",
      "<s> both our friends and now every one says the last time anybody saw us beating let’s say china in a certain way i wish i werent doing this but our country is in serious trouble and would be great </s>\n"
     ]
    }
   ],
   "source": [
    "#Input 1 for unigram 2 for bigram 3 for trigram etc\n",
    "\n",
    "N= int(input())\n",
    "print(\"\\n Printing output \\n\")\n",
    "sent_list = new_list[:]\n",
    "train_sent_list, test_sent_list = sent_list[:1000],sent_list[1000:]\n",
    "freq_count = ngrams(train_sent_list, N)\n",
    "print(generator(N, freq_count))\n",
    "print(generator(N, freq_count))\n",
    "print(generator(N, freq_count))\n",
    "print(generator(N, freq_count))\n",
    "print(generator(N, freq_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_perp = []\n",
    "\n",
    "for sent in test_sent_list:\n",
    "    ngrams = []\n",
    "    tokens = sent.split()\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = ' '.join(ngram[:-1])\n",
    "        last_token = str(ngram[-1])\n",
    "\n",
    "        if token_seq not in freq_count:\n",
    "            custom_perp.append(1)\n",
    "\n",
    "        elif last_token not in freq_count[token_seq]:\n",
    "            custom_perp.append(1)\n",
    "\n",
    "        else:\n",
    "            if N!=1:\n",
    "                choices = freq_count[token_seq].items()\n",
    "            else:\n",
    "                choices = freq_count[''].items()\n",
    "            pvals=[]\n",
    "            key_words=[]\n",
    "            total = sum(weight for choice, weight in choices)\n",
    "            custom_perp.append(freq_count[token_seq][last_token]/total)\n",
    "\n",
    "            \n",
    "def smoothing(custom_perp, c):\n",
    "    for i in range(len(custom_perp)):\n",
    "        if custom_perp[i]==1:\n",
    "            custom_perp[i]= c\n",
    "    return custom_perp\n",
    "\n",
    "# Smoothing\n",
    "c = min(custom_perp)\n",
    "custom_perp = smoothing(custom_perp,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 1 gram:  378.04495410782584\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 2 gram:  171.27055085484562\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 3 gram:  75.76329766782628\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 4 gram:  29.360625575730996\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability of generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The text is mostly human readable and grammatically correct some of flaws is that some part of the sentences are getting repeated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('speech.txt', 'r', encoding='utf-8') as file:\n",
    "    data = file.read().replace('\\n', '')\n",
    "    \n",
    "    \n",
    "data=data.encode('ascii', 'ignore').decode('ascii')\n",
    "data=data.lower()\n",
    "x=re.split(r'(\\.|,)', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=[]\n",
    "for i in x:\n",
    "    if(i != '.' and i!=',' and i !='' and 'speech' not in i):\n",
    "        final_data.append(i.strip())\n",
    "\n",
    "train_data=final_data[:20000]\n",
    "test_data=final_data[20000:]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(final_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "input_sequences = []\n",
    "for i in train_data:\n",
    "    token_list = tokenizer.texts_to_sequences([i])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences,   \n",
    "                          maxlen=15, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "label = utils.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 14, 100)           607000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6070)              1827070   \n",
      "=================================================================\n",
      "Total params: 2,554,370\n",
      "Trainable params: 2,554,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "117858/117858 [==============================] - 103s 870us/step - loss: 5.9464 - acc: 0.1025\n",
      "Epoch 2/8\n",
      "117858/117858 [==============================] - 58s 492us/step - loss: 5.3804 - acc: 0.1610\n",
      "Epoch 3/8\n",
      "117858/117858 [==============================] - 50s 422us/step - loss: 5.2277 - acc: 0.1788\n",
      "Epoch 4/8\n",
      "117858/117858 [==============================] - 51s 429us/step - loss: 5.1182 - acc: 0.1927\n",
      "Epoch 5/8\n",
      "117858/117858 [==============================] - 53s 446us/step - loss: 5.0088 - acc: 0.2045\n",
      "Epoch 6/8\n",
      "117858/117858 [==============================] - 51s 435us/step - loss: 4.9018 - acc: 0.2141\n",
      "Epoch 7/8\n",
      "117858/117858 [==============================] - 51s 434us/step - loss: 4.8016 - acc: 0.2209\n",
      "Epoch 8/8\n",
      "117858/117858 [==============================] - 51s 435us/step - loss: 4.7387 - acc: 0.2284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23fe9995940>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_len = 14\n",
    "model_rnn=Sequential()\n",
    "model_rnn.add(Embedding(total_words, 100, input_length=input_len))\n",
    "model_rnn.add(SimpleRNN(300))\n",
    "model_rnn.add(Dropout(0.2))\n",
    "model_rnn.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model_rnn.summary()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights_rnn.hdf5', verbose=1, save_best_only=True)\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model_rnn.fit(predictors, label, batch_size=128, epochs=8, verbose=1,callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't  know that i have a lot of money\n",
      "how do you know what i want to do\n",
      "that was a great guy with me\n",
      "trump is going to be a lot of money\n",
      "he said i was a great guy\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"i don't \", 8, 15, model_rnn))\n",
    "print(generate_text(\"how\", 8, 15, model_rnn))\n",
    "print(generate_text(\"that was\", 5, 15, model_rnn))\n",
    "print(generate_text(\"trump\", 8, 15, model_rnn))\n",
    "print(generate_text(\"he\", 6, 15, model_rnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23882, 15)\n",
      "Test score:  5.2604871629201115\n",
      "Test accuracy:  0.19659157524994594\n",
      "Perplexity  192.57528398691878\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "output_sequences = []\n",
    "for line in test_data:\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        output_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in output_sequences])\n",
    "output_sequences = np.array(pad_sequences(output_sequences,   \n",
    "                          maxlen=15, padding='pre'))\n",
    "\n",
    "print(output_sequences.shape)\n",
    "x_test, y_test = output_sequences[:,:-1],output_sequences[:,-1]\n",
    "y_test = utils.to_categorical(y_test, num_classes=total_words)\n",
    "\n",
    "score = model_rnn.evaluate(x_test, y_test, verbose=False) \n",
    "print('Test score: ', score[0])    \n",
    "print('Test accuracy: ', score[1])\n",
    "print(\"Perplexity \", np.exp(score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 14, 100)           607000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               481200    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6070)              1827070   \n",
      "=================================================================\n",
      "Total params: 2,915,270\n",
      "Trainable params: 2,915,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "117858/117858 [==============================] - 87s 740us/step - loss: 6.0621 - acc: 0.0761\n",
      "Epoch 2/8\n",
      "117858/117858 [==============================] - 86s 731us/step - loss: 5.2582 - acc: 0.1484\n",
      "Epoch 3/8\n",
      "117858/117858 [==============================] - 89s 758us/step - loss: 4.8766 - acc: 0.1746\n",
      "Epoch 4/8\n",
      "117858/117858 [==============================] - 91s 773us/step - loss: 4.6273 - acc: 0.1911\n",
      "Epoch 5/8\n",
      "117858/117858 [==============================] - 91s 772us/step - loss: 4.4199 - acc: 0.2052\n",
      "Epoch 6/8\n",
      "117858/117858 [==============================] - 92s 778us/step - loss: 4.2414 - acc: 0.2172\n",
      "Epoch 7/8\n",
      "117858/117858 [==============================] - 91s 769us/step - loss: 4.0775 - acc: 0.2287\n",
      "Epoch 8/8\n",
      "117858/117858 [==============================] - 90s 767us/step - loss: 3.9280 - acc: 0.2396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23fe9b30198>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_len = 14\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=input_len))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(predictors, label, batch_size=128, epochs=8, verbose=1,callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't want to do it because i dont want to do it\n",
      "how does you think its going to be a wall\n",
      "that was a disaster that was a disaster\n",
      "trump is going to be a wall\n",
      "he was a disaster for the united states\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"i don't\", 11, 15, model))\n",
    "print(generate_text(\"how\", 9, 15, model))\n",
    "print(generate_text(\"that was\", 6, 15, model))\n",
    "print(generate_text(\"trump\", 6, 15, model))\n",
    "print(generate_text(\"he\", 7, 15, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score:  5.1948463521244514\n",
      "Test accuracy:  0.20638974960720247\n",
      "Perplexity  180.34043174397908\n"
     ]
    }
   ],
   "source": [
    "output_sequences = []\n",
    "for line in test_data:\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        output_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in output_sequences])\n",
    "output_sequences = np.array(pad_sequences(output_sequences,   \n",
    "                          maxlen=15, padding='pre'))\n",
    "\n",
    "x_test, y_test = output_sequences[:,:-1],output_sequences[:,-1]\n",
    "y_test = utils.to_categorical(y_test, num_classes=total_words)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=False)\n",
    "print('Test score: ', score[0])    \n",
    "print('Test accuracy: ', score[1])\n",
    "print(\"Perplexity \", np.exp(score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability of neural generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The text is quite readable but at some places it is grammatically wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Neural Network better than Classical Approach, if so why? If not why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as perplexity of the model is concerned LSTM works better than RNN and unigram. On the other hand bigram works better than LSTM.\n",
    "For our dataset classical nlp techniques works better than neural network approach as far as grammatical correct sentences are concerned. But overall, I would still say neural network method works better than classical approach because it is able to generalize better. It can produce sentences that it has never seen, also it takes the context of all of the previous words in the sentences to produce output and hence more probablity of producing sematically correct sentences but in the case of n-grams it takes the context of only previous n-words. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
